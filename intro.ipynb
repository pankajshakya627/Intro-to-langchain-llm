{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! pip install langchain\\n! pip install openai\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "! pip install langchain\n",
    "! pip install openai\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config.API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# llm_opemAI = OpenAI(temperature=0.9)\n",
    "# print(llm_opemAI.predict(\"What is the Capital of india?\"))\n",
    "# print(llm_opemAI(\"What would be a good company name for a company that makes colorful socks?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Socktastic!\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"Programming_language\"],\n",
    "    template=\"What are the top 5 Programming language youtube channel to learn {Programming_language}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the top 5 Programming language youtube channel to learn python\n",
      "\n",
      "\n",
      "1. Corey Schafer\n",
      "2. thenewboston\n",
      "3. Derek Banas\n",
      "4. Clever Programmer\n",
      "5. Python Programmer\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Programming_language=\"python\"))\n",
    "print(llm(prompt.format(Programming_language=\"python\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "Sentiment: Is the text is Neutral, Positive or negative\n",
    "Subject: What is the subject text about? Use excatly one word\n",
    "\n",
    "Format the output in proper JSON with following keys:\n",
    "sentiment\n",
    "subject\n",
    "text: {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n{\\n    \"sentiment\": \"Positive\",\\n    \"subject\": \"Learning\",\\n    \"text\": \"I purchase ChatGPT course, it was very much helpful for my learning.\"\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.predict(input=\"I purchase ChatGPT course, it was very much helpful for my learning.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USe of ResponseSchema, Template, Chain and OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "sentiment_schema = ResponseSchema(name=\"sentiment\", description=\"Is the text Neutral, Positive or negative ? Only provide single word answere\")\n",
    "subject_schema = ResponseSchema(name=\"subject\", description=\"What subject is text about ? Use only single word answere\")\n",
    "price_schema = ResponseSchema(name=\"price\", description=\"How expensive was the product ? Use 'price not availble' in the text if price is not present else give price in float\")\n",
    "response_scheme = [sentiment_schema, subject_schema, price_schema]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"sentiment\": string  // Is the text Neutral, Positive or negative ? Only provide single word answere\\n\\t\"subject\": string  // What subject is text about ? Use only single word answere\\n\\t\"price\": string  // How expensive was the product ? Use \\'price not availble\\' in the text if price is not present else give price in float\\n}\\n```'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StructuredOutputParser.from_response_schemas(response_scheme)\n",
    "format_instruction = parser.get_format_instructions()\n",
    "format_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n\\t\"sentiment\": \"Positive\",\\n\\t\"subject\": \"Course\",\\n\\t\"price\": 299.98\\n}\\n```\\n\\nInterpretation: The text is expressing a positive sentiment about a course that was purchased for $299.98. The subject of the text is the course.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "Sentiment: Is the text is Neutral, Positive or negative\n",
    "Subject: What is the subject text about? Use excatly one word\n",
    "\n",
    "Format the output in proper JSON with following keys:\n",
    "sentiment\n",
    "subject\n",
    "text: {input}\n",
    "{format_instruction}\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "format_instruction = parser.get_format_instructions()\n",
    "\n",
    "message = prompt.format_messages(input=\"I purchase ChatGPT course in $ 299.98, it was very much helpful for my learning. \", format_instruction=format_instruction)\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response  = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n\\t\"sentiment\": \"Positive\",\\n\\t\"subject\": \"Course\",\\n\\t\"price\": 299.98\\n}\\n```' additional_kwargs={} example=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n\\t\"sentiment\": \"Positive\",\\n\\t\"subject\": \"Course\",\\n\\t\"price\": 299.98\\n}\\n```', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "Sentiment: Is the text is Neutral, Positive or negative\n",
    "Subject: What is the subject text about? Use excatly one word\n",
    "\n",
    "Return only JSON format output. Do not add any extara Interpretaion or text.\n",
    "text: {input}\n",
    "{format_instruction}\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "format_instruction = parser.get_format_instructions()\n",
    "\n",
    "message = prompt.format_messages(input=\"I purchase ChatGPT course in $ 299.98, it was very much helpful for my learning. \", format_instruction=format_instruction)\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response  = chat(message)\n",
    "print(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'Positive', 'subject': 'Course', 'price': 299.98}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Definition**: Chains are one of the fundamental building blocks of this library LangChain.\n",
    "\n",
    "The official definition of chains is as follows:\n",
    "\n",
    "> A chain is composed of links, which can be either primitives or other chains. Primitives can be prompts, LLMs, utilities, or other chains.\n",
    "\n",
    "So, a chain essentially functions as a pipeline that processes an input using a specific combination of primitives. In simpler terms, it can be seen as a 'step' that performs a defined set of operations on an input and produces a result. Chains can range from a prompt-based pass through a large language model (LLM) to applying a Python function to a text.\n",
    "\n",
    "Chains are categorized into three types: Utility chains, Generic chains, and Combine Documents chains. For this discussion, we will focus on the first two, as the third type is more specialized and will be covered later.\n",
    "\n",
    "1. Utility Chains: These chains are commonly used to extract specific answers from an LLM with a narrow purpose. They are readily available for immediate use.\n",
    "2. Generic Chains: These chains serve as building blocks for other chains but cannot be used independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import LLMChain, MultiPromptChain\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import inspect\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"sentiment\": \"Neutral\",\n",
      "    \"subject\": \"Java\",\n",
      "    \"text\": \"Java is use in most of complex web applications\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "Sentiment: Is the text is Neutral, Positive or negative\n",
    "Subject: What is the subject text about? Use excatly one word\n",
    "\n",
    "Format the output in proper JSON with following keys:\n",
    "sentiment\n",
    "subject\n",
    "text: {input}\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(temperature=0.0)\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "print(chain.predict(input =\"Java is use in most of complex web applications\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "13**.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 264 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  verbose=True to see what the different steps in the chain are!\n",
    "llm_math = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result\n",
    "\n",
    "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The question we send as input to the chain is not the only input that the LLM receives. The input is inserted into a broader context, which provides specific instructions on how to interpret the input we send. This broader context is known as a _prompt_. Now, let's examine what the prompt for this chain is!\n",
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 17 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n2.907'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above answere is wrong\n",
    "\n",
    "Herein lies the power of prompting and one of our most significant insights thus far:\n",
    "\n",
    "Insight: By utilizing prompts strategically, we can guide the behavior of the LLM, explicitly programming it to avoid common pitfalls and behave in a specific manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4116004626599237"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13**(.3432) ## Here is teh correct answere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, str],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
      "        _run_manager.on_text(inputs[self.input_key])\n",
      "        llm_output = self.llm_chain.predict(\n",
      "            question=inputs[self.input_key],\n",
      "            stop=[\"```output\"],\n",
      "            callbacks=_run_manager.get_child(),\n",
      "        )\n",
      "        return self._process_llm_result(llm_output, _run_manager)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The chain not only processes an input through the LLM but also incorporates Python code compilation at a later stage.\n",
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)\n",
    "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)\n",
    "\n",
    "\n",
    "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Chains provide a powerful mechanism for integrating various components into a unified application.\n",
    "\n",
    "For instance, we can construct a chain that receives user input, \n",
    "applies a formatting PromptTemplate to it,  \n",
    "\n",
    "and subsequently feeds the formatted response into an LLM. Complex chains can be built by combining multiple chains \n",
    "\n",
    "or by incorporating chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 168 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nChains be like a strong tool for puttin' together different pieces to make one big app. Ya know? Like, we can take user input, style it up with a PromptTemplate, then feed it to an LLM. We can even get more complex by mixin' multiple chains or addin' other components. Word.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 163 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nChains are a great way to combine different components into a single application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then sends the formatted output to an LLM. We can also create more complex chains by combining multiple chains or by adding chains to other components.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain, {'text': input_text, 'style': 'python professor'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  langchain-hub\n",
    "\n",
    "Loading a chain from the Langchain hub is a straightforward process. It involves locating the desired chain within the repository and utilizing the `load_chain` function with the appropriate path. Additionally, there are other functions available such as `load_prompt` and `initialize_agent`, which we will discuss in detail at a later stage. Now, let's explore how we can accomplish this with our previously mentioned LLMMathChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math_chain.verbose = False\n",
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
